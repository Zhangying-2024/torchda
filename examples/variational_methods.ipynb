{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3DVar and 4DVar implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from deepda import apply_3DVar, apply_4DVar, forwardModel_r\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(x: torch.Tensor):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = torch.eye(3, device=\"cuda\")\n",
    "R = torch.eye(3, device=\"cuda\")\n",
    "y = torch.tensor([10., 20., 30.], device=\"cuda\")\n",
    "xb = torch.zeros_like(y, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 0, Norm of J gradient: 74.83314514160156\n",
      "Iterations: 1, Norm of J gradient: 62.22539520263672\n",
      "Iterations: 2, Norm of J gradient: 50.396549224853516\n",
      "Iterations: 3, Norm of J gradient: 39.807682037353516\n",
      "Iterations: 4, Norm of J gradient: 30.810375213623047\n",
      "Iterations: 5, Norm of J gradient: 23.481937408447266\n",
      "Iterations: 6, Norm of J gradient: 18.005842208862305\n",
      "Iterations: 7, Norm of J gradient: 14.986854553222656\n",
      "Iterations: 8, Norm of J gradient: 14.758964538574219\n",
      "Iterations: 9, Norm of J gradient: 16.46552276611328\n",
      "Iterations: 10, Norm of J gradient: 18.841018676757812\n",
      "Iterations: 11, Norm of J gradient: 21.076297760009766\n",
      "Iterations: 12, Norm of J gradient: 22.750995635986328\n",
      "Iterations: 13, Norm of J gradient: 23.645172119140625\n",
      "Iterations: 14, Norm of J gradient: 23.67448616027832\n",
      "Iterations: 15, Norm of J gradient: 22.87781524658203\n",
      "Iterations: 16, Norm of J gradient: 21.400211334228516\n",
      "Iterations: 17, Norm of J gradient: 19.463558197021484\n",
      "Iterations: 18, Norm of J gradient: 17.333078384399414\n",
      "Iterations: 19, Norm of J gradient: 15.27811336517334\n",
      "Iterations: 20, Norm of J gradient: 13.517358779907227\n",
      "Iterations: 21, Norm of J gradient: 12.152778625488281\n",
      "Iterations: 22, Norm of J gradient: 11.135985374450684\n",
      "Iterations: 23, Norm of J gradient: 10.3181734085083\n",
      "Iterations: 24, Norm of J gradient: 9.558642387390137\n",
      "Iterations: 25, Norm of J gradient: 8.810657501220703\n",
      "Iterations: 26, Norm of J gradient: 8.144088745117188\n",
      "Iterations: 27, Norm of J gradient: 7.706360340118408\n",
      "Iterations: 28, Norm of J gradient: 7.6238861083984375\n",
      "Iterations: 29, Norm of J gradient: 7.890484809875488\n",
      "Iterations: 30, Norm of J gradient: 8.353668212890625\n",
      "Iterations: 31, Norm of J gradient: 8.80859088897705\n",
      "Iterations: 32, Norm of J gradient: 9.087627410888672\n",
      "Iterations: 33, Norm of J gradient: 9.093770027160645\n",
      "Iterations: 34, Norm of J gradient: 8.797890663146973\n",
      "Iterations: 35, Norm of J gradient: 8.223015785217285\n",
      "Iterations: 36, Norm of J gradient: 7.425343990325928\n",
      "Iterations: 37, Norm of J gradient: 6.4759697914123535\n",
      "Iterations: 38, Norm of J gradient: 5.446540355682373\n",
      "Iterations: 39, Norm of J gradient: 4.404730796813965\n",
      "Iterations: 40, Norm of J gradient: 3.428694009780884\n",
      "Iterations: 41, Norm of J gradient: 2.6526801586151123\n",
      "Iterations: 42, Norm of J gradient: 2.312659740447998\n",
      "Iterations: 43, Norm of J gradient: 2.554642677307129\n",
      "Iterations: 44, Norm of J gradient: 3.1612660884857178\n",
      "Iterations: 45, Norm of J gradient: 3.834031581878662\n",
      "Iterations: 46, Norm of J gradient: 4.388464450836182\n",
      "Iterations: 47, Norm of J gradient: 4.721522808074951\n",
      "Iterations: 48, Norm of J gradient: 4.780935287475586\n",
      "Iterations: 49, Norm of J gradient: 4.5552191734313965\n",
      "Iterations: 50, Norm of J gradient: 4.0688958168029785\n",
      "Iterations: 51, Norm of J gradient: 3.3767545223236084\n",
      "Iterations: 52, Norm of J gradient: 2.5570077896118164\n",
      "Iterations: 53, Norm of J gradient: 1.7092158794403076\n",
      "Iterations: 54, Norm of J gradient: 0.9894399642944336\n",
      "Iterations: 55, Norm of J gradient: 0.7761785984039307\n",
      "Iterations: 56, Norm of J gradient: 1.1524486541748047\n",
      "Iterations: 57, Norm of J gradient: 1.6070059537887573\n",
      "Iterations: 58, Norm of J gradient: 1.9683127403259277\n",
      "Iterations: 59, Norm of J gradient: 2.2085928916931152\n",
      "Iterations: 60, Norm of J gradient: 2.328948974609375\n",
      "Iterations: 61, Norm of J gradient: 2.335535764694214\n",
      "Iterations: 62, Norm of J gradient: 2.2345221042633057\n",
      "Iterations: 63, Norm of J gradient: 2.034461259841919\n",
      "Iterations: 64, Norm of J gradient: 1.7518410682678223\n",
      "Iterations: 65, Norm of J gradient: 1.417224645614624\n",
      "Iterations: 66, Norm of J gradient: 1.0823256969451904\n",
      "Iterations: 67, Norm of J gradient: 0.8295601606369019\n",
      "Iterations: 68, Norm of J gradient: 0.7510495781898499\n",
      "Iterations: 69, Norm of J gradient: 0.8318743705749512\n",
      "Iterations: 70, Norm of J gradient: 0.9557403326034546\n",
      "Iterations: 71, Norm of J gradient: 1.0455774068832397\n",
      "Iterations: 72, Norm of J gradient: 1.0808230638504028\n",
      "Iterations: 73, Norm of J gradient: 1.0716406106948853\n",
      "Iterations: 74, Norm of J gradient: 1.039773941040039\n",
      "Iterations: 75, Norm of J gradient: 1.003038763999939\n",
      "Iterations: 76, Norm of J gradient: 0.9651524424552917\n",
      "Iterations: 77, Norm of J gradient: 0.9166102409362793\n",
      "Iterations: 78, Norm of J gradient: 0.8448995351791382\n",
      "Iterations: 79, Norm of J gradient: 0.7449327707290649\n",
      "Iterations: 80, Norm of J gradient: 0.6242212057113647\n",
      "Iterations: 81, Norm of J gradient: 0.5036988854408264\n",
      "Iterations: 82, Norm of J gradient: 0.41533225774765015\n",
      "Iterations: 83, Norm of J gradient: 0.38731545209884644\n",
      "Iterations: 84, Norm of J gradient: 0.4142494201660156\n",
      "Iterations: 85, Norm of J gradient: 0.46401774883270264\n",
      "Iterations: 86, Norm of J gradient: 0.512049674987793\n",
      "Iterations: 87, Norm of J gradient: 0.5473591685295105\n",
      "Iterations: 88, Norm of J gradient: 0.5645383596420288\n",
      "Iterations: 89, Norm of J gradient: 0.5585201382637024\n",
      "Iterations: 90, Norm of J gradient: 0.524054765701294\n",
      "Iterations: 91, Norm of J gradient: 0.457940936088562\n",
      "Iterations: 92, Norm of J gradient: 0.36151036620140076\n",
      "Iterations: 93, Norm of J gradient: 0.2420547753572464\n",
      "Iterations: 94, Norm of J gradient: 0.11510682106018066\n",
      "Iterations: 95, Norm of J gradient: 0.06356871128082275\n",
      "Iterations: 96, Norm of J gradient: 0.16081561148166656\n",
      "Iterations: 97, Norm of J gradient: 0.25039124488830566\n",
      "Iterations: 98, Norm of J gradient: 0.3100701868534088\n",
      "Iterations: 99, Norm of J gradient: 0.33699482679367065\n",
      "Iterations: 100, Norm of J gradient: 0.3340838849544525\n",
      "Iterations: 101, Norm of J gradient: 0.30745530128479004\n",
      "Iterations: 102, Norm of J gradient: 0.2642608880996704\n",
      "Iterations: 103, Norm of J gradient: 0.21113979816436768\n",
      "Iterations: 104, Norm of J gradient: 0.15424013137817383\n",
      "Iterations: 105, Norm of J gradient: 0.10234743356704712\n",
      "Iterations: 106, Norm of J gradient: 0.07663651555776596\n",
      "Iterations: 107, Norm of J gradient: 0.09604308754205704\n",
      "Iterations: 108, Norm of J gradient: 0.13264146447181702\n",
      "Iterations: 109, Norm of J gradient: 0.1630903035402298\n",
      "Iterations: 110, Norm of J gradient: 0.17926977574825287\n",
      "Iterations: 111, Norm of J gradient: 0.17942650616168976\n",
      "Iterations: 112, Norm of J gradient: 0.1658645123243332\n",
      "Iterations: 113, Norm of J gradient: 0.143877774477005\n",
      "Iterations: 114, Norm of J gradient: 0.12032341957092285\n",
      "Iterations: 115, Norm of J gradient: 0.10130556672811508\n",
      "Iterations: 116, Norm of J gradient: 0.08921520411968231\n",
      "Iterations: 117, Norm of J gradient: 0.08202420175075531\n",
      "Iterations: 118, Norm of J gradient: 0.07699748873710632\n",
      "Iterations: 119, Norm of J gradient: 0.07411816716194153\n",
      "Iterations: 120, Norm of J gradient: 0.07504764199256897\n",
      "Iterations: 121, Norm of J gradient: 0.07961844652891159\n",
      "Iterations: 122, Norm of J gradient: 0.08491919934749603\n",
      "Iterations: 123, Norm of J gradient: 0.08769814670085907\n",
      "Iterations: 124, Norm of J gradient: 0.08628332614898682\n",
      "Iterations: 125, Norm of J gradient: 0.080771304666996\n",
      "Iterations: 126, Norm of J gradient: 0.07229346036911011\n",
      "Iterations: 127, Norm of J gradient: 0.062060993164777756\n",
      "Iterations: 128, Norm of J gradient: 0.05082641914486885\n",
      "Iterations: 129, Norm of J gradient: 0.039376016706228256\n",
      "Iterations: 130, Norm of J gradient: 0.030239136889576912\n",
      "Iterations: 131, Norm of J gradient: 0.028891080990433693\n",
      "Iterations: 132, Norm of J gradient: 0.036349497735500336\n",
      "Iterations: 133, Norm of J gradient: 0.04614785313606262\n",
      "Iterations: 134, Norm of J gradient: 0.053237371146678925\n",
      "Iterations: 135, Norm of J gradient: 0.05527843162417412\n",
      "Iterations: 136, Norm of J gradient: 0.051720939576625824\n",
      "Iterations: 137, Norm of J gradient: 0.04334075003862381\n",
      "Iterations: 138, Norm of J gradient: 0.031846050173044205\n",
      "Iterations: 139, Norm of J gradient: 0.01960616558790207\n",
      "Iterations: 140, Norm of J gradient: 0.010777593590319157\n",
      "Iterations: 141, Norm of J gradient: 0.012894125655293465\n",
      "Iterations: 142, Norm of J gradient: 0.020132284611463547\n",
      "Iterations: 143, Norm of J gradient: 0.026391295716166496\n",
      "Iterations: 144, Norm of J gradient: 0.030468227341771126\n",
      "Iterations: 145, Norm of J gradient: 0.03195970505475998\n",
      "Iterations: 146, Norm of J gradient: 0.030674755573272705\n",
      "Iterations: 147, Norm of J gradient: 0.026734935119748116\n",
      "Iterations: 148, Norm of J gradient: 0.020766915753483772\n",
      "Iterations: 149, Norm of J gradient: 0.014100508764386177\n",
      "Iterations: 150, Norm of J gradient: 0.009371286258101463\n",
      "Iterations: 151, Norm of J gradient: 0.009897079318761826\n",
      "Iterations: 152, Norm of J gradient: 0.013126573525369167\n",
      "Iterations: 153, Norm of J gradient: 0.015641091391444206\n",
      "Iterations: 154, Norm of J gradient: 0.016702307388186455\n",
      "Iterations: 155, Norm of J gradient: 0.016619380563497543\n",
      "Iterations: 156, Norm of J gradient: 0.01593206822872162\n",
      "Iterations: 157, Norm of J gradient: 0.014954355545341969\n",
      "Iterations: 158, Norm of J gradient: 0.013671603985130787\n",
      "Iterations: 159, Norm of J gradient: 0.011983664706349373\n",
      "Iterations: 160, Norm of J gradient: 0.010022195056080818\n",
      "Iterations: 161, Norm of J gradient: 0.008255258202552795\n",
      "Iterations: 162, Norm of J gradient: 0.007290133275091648\n",
      "Iterations: 163, Norm of J gradient: 0.0072772689163684845\n",
      "Iterations: 164, Norm of J gradient: 0.007760767824947834\n",
      "Iterations: 165, Norm of J gradient: 0.008322350680828094\n",
      "Iterations: 166, Norm of J gradient: 0.008829713799059391\n",
      "Iterations: 167, Norm of J gradient: 0.009196024388074875\n",
      "Iterations: 168, Norm of J gradient: 0.009207655675709248\n",
      "Iterations: 169, Norm of J gradient: 0.008600609377026558\n",
      "Iterations: 170, Norm of J gradient: 0.0072298068553209305\n",
      "Iterations: 171, Norm of J gradient: 0.005180796142667532\n",
      "Iterations: 172, Norm of J gradient: 0.00286669097840786\n",
      "Iterations: 173, Norm of J gradient: 0.0017953136702999473\n",
      "Iterations: 174, Norm of J gradient: 0.0032873949967324734\n",
      "Iterations: 175, Norm of J gradient: 0.004875411279499531\n",
      "Iterations: 176, Norm of J gradient: 0.005844182334840298\n",
      "Iterations: 177, Norm of J gradient: 0.006127594970166683\n",
      "Iterations: 178, Norm of J gradient: 0.005813279654830694\n",
      "Iterations: 179, Norm of J gradient: 0.005033695138990879\n",
      "Iterations: 180, Norm of J gradient: 0.003903965698555112\n",
      "Iterations: 181, Norm of J gradient: 0.00255011860281229\n",
      "Iterations: 182, Norm of J gradient: 0.001266054343432188\n",
      "Iterations: 183, Norm of J gradient: 0.0012842860305681825\n",
      "Iterations: 184, Norm of J gradient: 0.0023608291521668434\n",
      "Iterations: 185, Norm of J gradient: 0.00323925307020545\n",
      "Iterations: 186, Norm of J gradient: 0.003665426280349493\n",
      "Iterations: 187, Norm of J gradient: 0.0036149953957647085\n",
      "Iterations: 188, Norm of J gradient: 0.0031967805698513985\n",
      "Iterations: 189, Norm of J gradient: 0.00260348548181355\n",
      "Iterations: 190, Norm of J gradient: 0.0020433294121176004\n",
      "Iterations: 191, Norm of J gradient: 0.0016736536053940654\n",
      "Iterations: 192, Norm of J gradient: 0.0015206210082396865\n",
      "Iterations: 193, Norm of J gradient: 0.0015202956274151802\n",
      "Iterations: 194, Norm of J gradient: 0.0016258398536592722\n",
      "Iterations: 195, Norm of J gradient: 0.001777558820322156\n",
      "Iterations: 196, Norm of J gradient: 0.0018941431771963835\n",
      "Iterations: 197, Norm of J gradient: 0.0019119017524644732\n",
      "Iterations: 198, Norm of J gradient: 0.001813836395740509\n",
      "Iterations: 199, Norm of J gradient: 0.0016391095705330372\n",
      "Iterations: 200, Norm of J gradient: 0.0014460922684520483\n",
      "Iterations: 201, Norm of J gradient: 0.0012519473675638437\n",
      "Iterations: 202, Norm of J gradient: 0.0010427895467728376\n",
      "Iterations: 203, Norm of J gradient: 0.0008310372941195965\n",
      "Iterations: 204, Norm of J gradient: 0.0007237502140924335\n",
      "Iterations: 205, Norm of J gradient: 0.0008319385815411806\n",
      "Iterations: 206, Norm of J gradient: 0.0010443652281537652\n",
      "Iterations: 207, Norm of J gradient: 0.0012047741329297423\n",
      "Iterations: 208, Norm of J gradient: 0.001230273861438036\n",
      "Iterations: 209, Norm of J gradient: 0.001110871322453022\n",
      "Iterations: 210, Norm of J gradient: 0.0008809824357740581\n",
      "Iterations: 211, Norm of J gradient: 0.0005992839578539133\n",
      "Iterations: 212, Norm of J gradient: 0.0003473050892353058\n",
      "Iterations: 213, Norm of J gradient: 0.00029036839259788394\n",
      "Iterations: 214, Norm of J gradient: 0.0004517961060628295\n",
      "Iterations: 215, Norm of J gradient: 0.0006287686992436647\n",
      "Iterations: 216, Norm of J gradient: 0.0007503447704948485\n",
      "Iterations: 217, Norm of J gradient: 0.0007857257733121514\n",
      "Iterations: 218, Norm of J gradient: 0.0007232876378111541\n",
      "Iterations: 219, Norm of J gradient: 0.000577725179027766\n",
      "Iterations: 220, Norm of J gradient: 0.00037880922900512815\n",
      "Iterations: 221, Norm of J gradient: 0.00020253851835150272\n",
      "Iterations: 222, Norm of J gradient: 0.00020595833484549075\n",
      "Iterations: 223, Norm of J gradient: 0.0003229226276744157\n",
      "Iterations: 224, Norm of J gradient: 0.00040558414184488356\n",
      "Iterations: 225, Norm of J gradient: 0.0004406305670272559\n",
      "Iterations: 226, Norm of J gradient: 0.0004369618836790323\n",
      "Iterations: 227, Norm of J gradient: 0.00041018196498043835\n",
      "Iterations: 228, Norm of J gradient: 0.0003617984475567937\n",
      "Iterations: 229, Norm of J gradient: 0.000299356528557837\n",
      "Iterations: 230, Norm of J gradient: 0.0002356482727918774\n",
      "Iterations: 231, Norm of J gradient: 0.0002004803973250091\n",
      "Iterations: 232, Norm of J gradient: 0.00019986246479675174\n",
      "Iterations: 233, Norm of J gradient: 0.00021690168068744242\n",
      "Iterations: 234, Norm of J gradient: 0.00022911219275556505\n",
      "Iterations: 235, Norm of J gradient: 0.00023537795641459525\n",
      "Iterations: 236, Norm of J gradient: 0.00024148118973243982\n",
      "Iterations: 237, Norm of J gradient: 0.00024192519776988775\n",
      "Iterations: 238, Norm of J gradient: 0.0002336015459150076\n",
      "Iterations: 239, Norm of J gradient: 0.00020221494196448475\n",
      "Iterations: 240, Norm of J gradient: 0.0001499423524364829\n",
      "Iterations: 241, Norm of J gradient: 9.244248212780803e-05\n",
      "Iterations: 242, Norm of J gradient: 7.574359915452078e-05\n",
      "Iterations: 243, Norm of J gradient: 0.0001109545846702531\n",
      "Iterations: 244, Norm of J gradient: 0.00014744681539013982\n",
      "Iterations: 245, Norm of J gradient: 0.00016863594646565616\n",
      "Iterations: 246, Norm of J gradient: 0.00016953959129750729\n",
      "Iterations: 247, Norm of J gradient: 0.00015487153723370284\n",
      "Iterations: 248, Norm of J gradient: 0.00012571149272844195\n",
      "Iterations: 249, Norm of J gradient: 8.261264156317338e-05\n",
      "Iterations: 250, Norm of J gradient: 3.480584564385936e-05\n",
      "Iterations: 251, Norm of J gradient: 2.4722063244553283e-05\n",
      "Iterations: 252, Norm of J gradient: 6.719175144098699e-05\n",
      "Iterations: 253, Norm of J gradient: 9.667454287409782e-05\n",
      "Iterations: 254, Norm of J gradient: 0.00011064266436733305\n",
      "Iterations: 255, Norm of J gradient: 0.00010769343498395756\n",
      "Iterations: 256, Norm of J gradient: 9.220605716109276e-05\n",
      "Iterations: 257, Norm of J gradient: 7.159547385526821e-05\n",
      "Iterations: 258, Norm of J gradient: 5.1745071687037125e-05\n",
      "Iterations: 259, Norm of J gradient: 4.0008882933761925e-05\n",
      "Iterations: 260, Norm of J gradient: 4.112979513593018e-05\n",
      "Iterations: 261, Norm of J gradient: 5.188549403101206e-05\n",
      "Iterations: 262, Norm of J gradient: 6.06764733674936e-05\n",
      "Iterations: 263, Norm of J gradient: 6.212805601535365e-05\n",
      "Iterations: 264, Norm of J gradient: 6.082618347136304e-05\n",
      "Iterations: 265, Norm of J gradient: 5.438452717498876e-05\n",
      "Iterations: 266, Norm of J gradient: 4.4649885239778087e-05\n",
      "Iterations: 267, Norm of J gradient: 3.64398438250646e-05\n",
      "Iterations: 268, Norm of J gradient: 3.197298792656511e-05\n",
      "Iterations: 269, Norm of J gradient: 3.0097453418420628e-05\n",
      "Iterations: 270, Norm of J gradient: 2.5589766664779745e-05\n",
      "Iterations: 271, Norm of J gradient: 2.9051869205432013e-05\n",
      "Iterations: 272, Norm of J gradient: 3.5169770853826776e-05\n",
      "Iterations: 273, Norm of J gradient: 3.698486398207024e-05\n",
      "Iterations: 274, Norm of J gradient: 3.542742706486024e-05\n",
      "Iterations: 275, Norm of J gradient: 3.03382366837468e-05\n",
      "Iterations: 276, Norm of J gradient: 2.4425980882369913e-05\n",
      "Iterations: 277, Norm of J gradient: 1.7993890651268885e-05\n",
      "Iterations: 278, Norm of J gradient: 1.2212990441184957e-05\n",
      "Iterations: 279, Norm of J gradient: 1.078959303413285e-05\n",
      "Iterations: 280, Norm of J gradient: 1.748113936628215e-05\n",
      "Iterations: 281, Norm of J gradient: 2.2324942619889043e-05\n",
      "Iterations: 282, Norm of J gradient: 2.2967518816585653e-05\n",
      "Iterations: 283, Norm of J gradient: 2.3515372959082015e-05\n",
      "Iterations: 284, Norm of J gradient: 1.7993890651268885e-05\n",
      "Iterations: 285, Norm of J gradient: 1.2212990441184957e-05\n",
      "Iterations: 286, Norm of J gradient: 7.864199687901419e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 5.0000, 10.0000, 15.0000], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_3DVar(H, B, R, xb, y, learning_rate=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the control parameters here\n",
    "rayleigh = 35\n",
    "prandtl = 10.\n",
    "b = 8./3.\n",
    "# rayleigh = 0.\n",
    "# prandtl = 0.\n",
    "# b = 0.\n",
    "# initial condition for the true reference trajectory\n",
    "x0 = torch.tensor([0., 1., 2.], device=\"cuda\")\n",
    "\n",
    "# integration time parameter\n",
    "dt = 1.e-3      # This is time step size\n",
    "T = 2.         # Total integration time, can be as short as 10 to speed things up\n",
    "n_steps = ceil(T / dt)\n",
    "time = torch.linspace(0., T, n_steps + 1, device=\"cuda\")  # array of discrete times\n",
    "\n",
    "# numerical integration given initial conditions and control parameters\n",
    "xt = forwardModel_r(x0, time, rayleigh, prandtl, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which variables do we observe?\n",
    "WhichVariablesAreObserved = torch.tensor([1, 1, 1])\n",
    "#  Determines which variables are available to\n",
    "#  the EnKF. For example:\n",
    "#  WhichVariablesAreObserved = [1 1 1];\n",
    "#  means: X, Y, Z are observed\n",
    "#  WhichVariablesAreObserved = [1 0 1];\n",
    "#  means: X and Z are observed\n",
    "#  WhichVariablesAreObserved = [1 0 0];\n",
    "# means: X is observed\n",
    "sigobs = 2.  # standard deviation of the observation noise\n",
    "# We generate the synthetic data\n",
    "#  Construct observation matrix H\n",
    "#  ........................................................................\n",
    "# H = torch.diag(WhichVariablesAreObserved)\n",
    "y_size = int(WhichVariablesAreObserved.sum())\n",
    "# How often do we observe the true state?\n",
    "dtobs = 0.5  # time between observations\n",
    "nobs = ceil(T / dtobs) - 1  # number of times observations are performed\n",
    "# no observation at t=0\n",
    "gap = int(dtobs / dt)  # number of time steps between each observation\n",
    "time_obs = time[gap::gap]\n",
    "# Generate vector of observations\n",
    "y = torch.zeros((y_size, nobs), device=\"cuda\")\n",
    "R = torch.diag(torch.tile(torch.tensor(sigobs**2, device=\"cuda\"), (y_size,)))\n",
    "sqrt_s = torch.sqrt(R)\n",
    "# y = Hxt\n",
    "y = H(xt[:, gap::gap])\n",
    "# compute observation error\n",
    "noise = sqrt_s @ torch.randn(size=y.shape, device=\"cuda\")\n",
    "# y = Hxt + epsilon\n",
    "y = y + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 0, Norm of J gradient: 8.582450866699219\n",
      "Iterations: 1, Norm of J gradient: 3.3278417587280273\n",
      "Iterations: 2, Norm of J gradient: 2.696883201599121\n",
      "Iterations: 3, Norm of J gradient: 5.678588390350342\n",
      "Iterations: 4, Norm of J gradient: 5.940237522125244\n",
      "Iterations: 5, Norm of J gradient: 4.417841911315918\n",
      "Iterations: 6, Norm of J gradient: 2.257413148880005\n",
      "Iterations: 7, Norm of J gradient: 1.4461159706115723\n",
      "Iterations: 8, Norm of J gradient: 2.8629040718078613\n",
      "Iterations: 9, Norm of J gradient: 3.8686680793762207\n",
      "Iterations: 10, Norm of J gradient: 4.043210983276367\n",
      "Iterations: 11, Norm of J gradient: 3.48136568069458\n",
      "Iterations: 12, Norm of J gradient: 2.4226934909820557\n",
      "Iterations: 13, Norm of J gradient: 1.3422902822494507\n",
      "Iterations: 14, Norm of J gradient: 1.4460490942001343\n",
      "Iterations: 15, Norm of J gradient: 2.3449459075927734\n",
      "Iterations: 16, Norm of J gradient: 2.9118547439575195\n",
      "Iterations: 17, Norm of J gradient: 2.8954708576202393\n",
      "Iterations: 18, Norm of J gradient: 2.355807065963745\n",
      "Iterations: 19, Norm of J gradient: 1.5474618673324585\n",
      "Iterations: 20, Norm of J gradient: 1.0447170734405518\n",
      "Iterations: 21, Norm of J gradient: 1.419373631477356\n",
      "Iterations: 22, Norm of J gradient: 1.9593788385391235\n",
      "Iterations: 23, Norm of J gradient: 2.203774929046631\n",
      "Iterations: 24, Norm of J gradient: 2.0707316398620605\n",
      "Iterations: 25, Norm of J gradient: 1.630591630935669\n",
      "Iterations: 26, Norm of J gradient: 1.0985522270202637\n",
      "Iterations: 27, Norm of J gradient: 0.9371168613433838\n",
      "Iterations: 28, Norm of J gradient: 1.2761256694793701\n",
      "Iterations: 29, Norm of J gradient: 1.594083309173584\n",
      "Iterations: 30, Norm of J gradient: 1.642073154449463\n",
      "Iterations: 31, Norm of J gradient: 1.4065725803375244\n",
      "Iterations: 32, Norm of J gradient: 1.0227400064468384\n",
      "Iterations: 33, Norm of J gradient: 0.8014873266220093\n",
      "Iterations: 34, Norm of J gradient: 0.9671075940132141\n",
      "Iterations: 35, Norm of J gradient: 1.2165141105651855\n",
      "Iterations: 36, Norm of J gradient: 1.3035961389541626\n",
      "Iterations: 37, Norm of J gradient: 1.1795674562454224\n",
      "Iterations: 38, Norm of J gradient: 0.9163647294044495\n",
      "Iterations: 39, Norm of J gradient: 0.7078255414962769\n",
      "Iterations: 40, Norm of J gradient: 0.7613103985786438\n",
      "Iterations: 41, Norm of J gradient: 0.9345657229423523\n",
      "Iterations: 42, Norm of J gradient: 1.0069456100463867\n",
      "Iterations: 43, Norm of J gradient: 0.917927086353302\n",
      "Iterations: 44, Norm of J gradient: 0.7288427352905273\n",
      "Iterations: 45, Norm of J gradient: 0.6043753027915955\n",
      "Iterations: 46, Norm of J gradient: 0.6670013666152954\n",
      "Iterations: 47, Norm of J gradient: 0.7869337201118469\n",
      "Iterations: 48, Norm of J gradient: 0.8185098767280579\n",
      "Iterations: 49, Norm of J gradient: 0.7324557304382324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0210,  0.9871,  1.6798], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_4DVar(nobs, time_obs, gap, forwardModel_r, H, B, R, x0, y, model_args=(rayleigh, prandtl, b), learning_rate=0.00725, max_iterations=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
